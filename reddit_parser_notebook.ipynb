{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Reddit Parser –∑ AI –ê–Ω–∞–ª—ñ–∑–æ–º\n",
        "\n",
        "–¶–µ–π notebook –¥–æ–∑–≤–æ–ª—è—î:\n",
        "- üìä –ü–∞—Ä—Å–∏—Ç–∏ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó —Ç–∞ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ Reddit\n",
        "- üß† –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –¥–∞–Ω—ñ —á–µ—Ä–µ–∑ LLM\n",
        "- üí° –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —ñ–¥–µ—ó –ø–æ—Å—Ç—ñ–≤\n",
        "- üìà –°—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—É –∞–Ω–∞–ª—ñ—Ç–∏–∫—É\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ —ñ–º–ø–æ—Ä—Ç–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π (–∑–∞–ø—É—Å—Ç—ñ—Ç—å –æ–¥–∏–Ω —Ä–∞–∑)\n",
        "%pip install praw python-dotenv requests openpyxl pandas openai httpx backoff nest-asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import praw\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import openai\n",
        "import backoff\n",
        "import nest_asyncio\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# –î–æ–∑–≤–æ–ª—è—î–º–æ –≤–∫–ª–∞–¥–µ–Ω—ñ event loops –¥–ª—è Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print(\"‚úÖ –Ü–º–ø–æ—Ä—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåç –ì–ª–æ–±–∞–ª—å–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéØ –ì–õ–û–ë–ê–õ–¨–ù–Ü –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø - –†–ï–î–ê–ì–£–ô–¢–ï –¢–£–¢\n",
        "\n",
        "# –°–∞–±—Ä–µ–¥–¥—ñ—Ç–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É\n",
        "TARGET_SUBREDDITS = [\n",
        "    'Python',           # Python programming\n",
        "    'MachineLearning',  # Machine learning\n",
        "    'programming',      # General programming\n",
        "    'artificial',       # Artificial intelligence\n",
        "    'technology'        # Technology\n",
        "]\n",
        "\n",
        "# –°–∞–±—Ä–µ–¥–¥—ñ—Ç–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É\n",
        "CONTENT_GENERATION_SUBREDDITS = [\n",
        "    'Python',\n",
        "    'learnpython',\n",
        "    'MachineLearning'\n",
        "]\n",
        "\n",
        "# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä—Å–∏–Ω–≥—É\n",
        "POSTS_PER_SUBREDDIT = 15  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Å—Ç—ñ–≤ –∑ –∫–æ–∂–Ω–æ–≥–æ —Å–∞–±—Ä–µ–¥–¥—ñ—Ç–∞\n",
        "SORT_BY = 'hot'  # 'hot', 'new', 'top', 'rising'\n",
        "COMMENTS_PER_POST = 10  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–æ –∫–æ–∂–Ω–æ–≥–æ –ø–æ—Å—Ç–∞\n",
        "TEXT_LIMIT = 2000  # –õ—ñ–º—ñ—Ç —Å–∏–º–≤–æ–ª—ñ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç—ñ–≤ (None = –±–µ–∑ –ª—ñ–º—ñ—Ç—É)\n",
        "\n",
        "# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è LLM\n",
        "LLM_MODEL = 'google/gemini-2.5-flash-lite-preview-06-17'\n",
        "MAX_CONCURRENT_REQUESTS = 30  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ (–∑–º–µ–Ω—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ)\n",
        "IDEAS_PER_SUBREDDIT = 2  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ–¥–µ–π –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Å–∞–±—Ä–µ–¥–¥—ñ—Ç–∞\n",
        "MAX_TOKENS = 2500  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ\n",
        "TIMEOUT_SECONDS = 90  # –ß–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ (—Å–µ–∫—É–Ω–¥–∏)\n",
        "ENABLE_CONTINUATION = True  # –£–≤—ñ–º–∫–Ω—É—Ç–∏ –¥–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –ø—Ä–∏ –æ–±—Ä—ñ–∑–∞–Ω–Ω—ñ\n",
        "\n",
        "# API –∫–ª—é—á—ñ (—Å—Ç–≤–æ—Ä—ñ—Ç—å —Ñ–∞–π–ª .env –∞–±–æ –≤–≤–µ–¥—ñ—Ç—å —Ç—É—Ç)\n",
        "REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID', 'your_client_id_here')\n",
        "REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET', 'your_client_secret_here')\n",
        "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', 'your_openrouter_key_here')\n",
        "\n",
        "print(f\"üéØ –ù–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ {len(TARGET_SUBREDDITS)} —Å–∞–±—Ä–µ–¥–¥—ñ—Ç—ñ–≤\")\n",
        "print(f\"üìä –ë—É–¥–µ —Å–ø–∞—Ä—Å–µ–Ω–æ {POSTS_PER_SUBREDDIT * len(TARGET_SUBREDDITS)} –ø–æ—Å—Ç—ñ–≤\")\n",
        "print(f\"üí° –ë—É–¥–µ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {IDEAS_PER_SUBREDDIT * len(CONTENT_GENERATION_SUBREDDITS)} —ñ–¥–µ–π\")\n",
        "print(f\"ü§ñ LLM –º–æ–¥–µ–ª—å: {LLM_MODEL}\")\n",
        "print(f\"üîß –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω—ñ–≤: {MAX_TOKENS}, Timeout: {TIMEOUT_SECONDS}s\")\n",
        "print(f\"üîÑ –î–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è: {'–£–≤—ñ–º–∫–Ω–µ–Ω–∞' if ENABLE_CONTINUATION else '–í–∏–º–∫–Ω–µ–Ω–∞'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä –ü–∞—Ä—Å–∏–Ω–≥ Reddit –¥–∞–Ω–∏—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_reddit_data(subreddits, posts_limit, sort_by, comments_limit, text_limit):\n",
        "    \"\"\"\n",
        "    –ü–∞—Ä—Å–∏–Ω–≥ Reddit –¥–∞–Ω–∏—Ö –∑ –∫–æ–º–µ–Ω—Ç–∞—Ä—è–º–∏\n",
        "    \"\"\"\n",
        "    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Reddit API\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=REDDIT_CLIENT_ID,\n",
        "        client_secret=REDDIT_CLIENT_SECRET,\n",
        "        user_agent='RedditParser/1.0'\n",
        "    )\n",
        "    \n",
        "    all_posts = []\n",
        "    all_comments = []\n",
        "    \n",
        "    for subreddit_name in subreddits:\n",
        "        print(f\"üìä –ü–∞—Ä—Å–∏–Ω–≥ r/{subreddit_name}...\")\n",
        "        \n",
        "        try:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            \n",
        "            # –í–∏–±—ñ—Ä –º–µ—Ç–æ–¥—É —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è\n",
        "            if sort_by == 'hot':\n",
        "                posts = subreddit.hot(limit=posts_limit)\n",
        "            elif sort_by == 'new':\n",
        "                posts = subreddit.new(limit=posts_limit)\n",
        "            elif sort_by == 'top':\n",
        "                posts = subreddit.top(limit=posts_limit)\n",
        "            else:\n",
        "                posts = subreddit.hot(limit=posts_limit)\n",
        "            \n",
        "            post_count = 0\n",
        "            for post in posts:\n",
        "                # –û–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É –ø–æ—Å—Ç–∞\n",
        "                selftext = post.selftext\n",
        "                if text_limit and len(selftext) > text_limit:\n",
        "                    selftext = selftext[:text_limit] + '...'\n",
        "                \n",
        "                post_data = {\n",
        "                    'post_id': post.id,\n",
        "                    'title': post.title,\n",
        "                    'author': str(post.author) if post.author else '[deleted]',\n",
        "                    'score': post.score,\n",
        "                    'upvote_ratio': post.upvote_ratio,\n",
        "                    'num_comments': post.num_comments,\n",
        "                    'created_utc': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'url': post.url,\n",
        "                    'permalink': f\"https://reddit.com{post.permalink}\",\n",
        "                    'selftext': selftext,\n",
        "                    'selftext_length': len(post.selftext),\n",
        "                    'subreddit': str(post.subreddit),\n",
        "                    'is_video': post.is_video,\n",
        "                    'over_18': post.over_18,\n",
        "                    'gilded': post.gilded\n",
        "                }\n",
        "                all_posts.append(post_data)\n",
        "                \n",
        "                # –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤ –¥–æ –ø–æ—Å—Ç–∞\n",
        "                if comments_limit > 0:\n",
        "                    try:\n",
        "                        post.comments.replace_more(limit=0)\n",
        "                        comment_count = 0\n",
        "                        \n",
        "                        for comment in post.comments.list():\n",
        "                            if comment_count >= comments_limit:\n",
        "                                break\n",
        "                                \n",
        "                            if hasattr(comment, 'body'):\n",
        "                                comment_body = comment.body\n",
        "                                if text_limit and len(comment_body) > text_limit:\n",
        "                                    comment_body = comment_body[:text_limit] + '...'\n",
        "                                \n",
        "                                comment_data = {\n",
        "                                    'comment_id': comment.id,\n",
        "                                    'post_id': post.id,\n",
        "                                    'author': str(comment.author) if comment.author else '[deleted]',\n",
        "                                    'body': comment_body,\n",
        "                                    'body_length': len(comment.body),\n",
        "                                    'score': comment.score,\n",
        "                                    'created_utc': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                                    'subreddit': subreddit_name,\n",
        "                                    'is_submitter': comment.is_submitter,\n",
        "                                    'gilded': comment.gilded\n",
        "                                }\n",
        "                                all_comments.append(comment_data)\n",
        "                                comment_count += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {e}\")\n",
        "                \n",
        "                post_count += 1\n",
        "            \n",
        "            print(f\"‚úÖ r/{subreddit_name}: {post_count} –ø–æ—Å—Ç—ñ–≤, {len([c for c in all_comments if c['subreddit'] == subreddit_name])} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤\")\n",
        "            time.sleep(1)  # –ó–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ —Å–∞–±—Ä–µ–¥–¥—ñ—Ç–∞–º–∏\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É r/{subreddit_name}: {e}\")\n",
        "    \n",
        "    return pd.DataFrame(all_posts), pd.DataFrame(all_comments)\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥—É\n",
        "print(\"üöÄ –ü–æ—á–∞—Ç–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥—É Reddit...\")\n",
        "posts_df, comments_df = parse_reddit_data(\n",
        "    TARGET_SUBREDDITS, \n",
        "    POSTS_PER_SUBREDDIT, \n",
        "    SORT_BY, \n",
        "    COMMENTS_PER_POST, \n",
        "    TEXT_LIMIT\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–∞—Ä—Å–∏–Ω–≥—É:\")\n",
        "print(f\"üìù –ü–æ—Å—Ç—ñ–≤: {len(posts_df)}\")\n",
        "print(f\"üí¨ –ö–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {len(comments_df)}\")\n",
        "print(f\"üè∑Ô∏è –°–∞–±—Ä–µ–¥–¥—ñ—Ç—ñ–≤: {posts_df['subreddit'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà –ë–∞–∑–æ–≤–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–æ–ø –ø–æ—Å—Ç—ñ–≤\n",
        "print(\"üèÜ –¢–û–ü-10 –ü–û–°–¢–Ü–í –ó–ê –†–ï–ô–¢–ò–ù–ì–û–ú:\")\n",
        "top_posts = posts_df.nlargest(10, 'score')[['title', 'score', 'subreddit', 'num_comments']]\n",
        "for i, (_, post) in enumerate(top_posts.iterrows(), 1):\n",
        "    print(f\"{i:2d}. {post['title'][:60]}... (Score: {post['score']}, r/{post['subreddit']})\")\n",
        "\n",
        "print(f\"\\nüí¨ –¢–û–ü-5 –ü–û–°–¢–Ü–í –ó–ê –ö–û–ú–ï–ù–¢–ê–†–Ø–ú–ò:\")\n",
        "top_commented = posts_df.nlargest(5, 'num_comments')[['title', 'num_comments', 'subreddit', 'score']]\n",
        "for i, (_, post) in enumerate(top_commented.iterrows(), 1):\n",
        "    print(f\"{i}. {post['title'][:50]}... ({post['num_comments']} –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤, r/{post['subreddit']})\")\n",
        "\n",
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∞–±—Ä–µ–¥–¥—ñ—Ç–∞–º\n",
        "print(f\"\\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–ê–ë–†–ï–î–î–Ü–¢–ê–ú:\")\n",
        "subreddit_stats = posts_df.groupby('subreddit').agg({\n",
        "    'score': ['mean', 'max'],\n",
        "    'num_comments': ['mean', 'max'],\n",
        "    'post_id': 'count'\n",
        "}).round(1)\n",
        "\n",
        "for subreddit in subreddit_stats.index:\n",
        "    stats = subreddit_stats.loc[subreddit]\n",
        "    print(f\"r/{subreddit}: {stats[('post_id', 'count')]} –ø–æ—Å—Ç—ñ–≤, —Å–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥: {stats[('score', 'mean')]}, –º–∞–∫—Å: {stats[('score', 'max')]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç–∏–ª—ñ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç—É\n",
        "\n",
        "–û–±–µ—Ä—ñ—Ç—å –æ–¥–Ω—É –∑ –≥–æ—Ç–æ–≤–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –±–∞–∂–∞–Ω–æ–≥–æ —Å—Ç–∏–ª—é:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç–∏–ª—ñ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç—É\n",
        "CONTENT_STYLES = {\n",
        "    \"human_relatable\": {\n",
        "        \"description\": \"üßë‚Äçüíª –õ—é–¥—è–Ω–∏–π —Ç–∞ —Å–ø—ñ–≤—á—É—Ç–ª–∏–≤–∏–π —Å—Ç–∏–ª—å\",\n",
        "        \"model\": \"anthropic/claude-3-haiku\",\n",
        "        \"temperature\": 0.8,\n",
        "        \"max_tokens\": 2000,\n",
        "        \"focus\": \"–æ—Å–æ–±–∏—Å—Ç—ñ —ñ—Å—Ç–æ—Ä—ñ—ó, –ø—Ä–æ–±–ª–µ–º–∏, —Å–ø—ñ–≤–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–Ω—è\"\n",
        "    },\n",
        "    \n",
        "    \"humorous_casual\": {\n",
        "        \"description\": \"üòÑ –ì—É–º–æ—Ä–∏—Å—Ç–∏—á–Ω–∏–π —Ç–∞ –Ω–µ–≤–∏–º—É—à–µ–Ω–∏–π\",\n",
        "        \"model\": \"openai/gpt-4o-mini\",\n",
        "        \"temperature\": 0.9,\n",
        "        \"max_tokens\": 1800,\n",
        "        \"focus\": \"–∂–∞—Ä—Ç–∏, –º–µ–º—ñ, –ª–µ–≥–∫–∏–π —Ç–æ–Ω, Reddit-—Å–ª–µ–Ω–≥\"\n",
        "    },\n",
        "    \n",
        "    \"technical_engaging\": {\n",
        "        \"description\": \"üîß –¢–µ—Ö–Ω—ñ—á–Ω–∏–π –∞–ª–µ –∑–∞—Ö–æ–ø–ª—é—é—á–∏–π\",\n",
        "        \"model\": \"google/gemini-2.0-flash-exp\",\n",
        "        \"temperature\": 0.6,\n",
        "        \"max_tokens\": 2500,\n",
        "        \"focus\": \"—Ç–µ—Ö–Ω—ñ—á–Ω—ñ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è, –ø–æ—Ä–∞–¥–∏, TIL —Ñ–æ—Ä–º–∞—Ç\"\n",
        "    },\n",
        "    \n",
        "    \"discussion_starter\": {\n",
        "        \"description\": \"üí¨ –ü—Ä–æ–≤–æ–∫—É—î –¥–∏—Å–∫—É—Å—ñ—ó\",\n",
        "        \"model\": \"anthropic/claude-3-sonnet\",\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 2200,\n",
        "        \"focus\": \"—Å–ø—ñ—Ä–Ω—ñ –¥—É–º–∫–∏, –ø–∏—Ç–∞–Ω–Ω—è, –¥–µ–±–∞—Ç–∏\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# –í–∏–±–µ—Ä—ñ—Ç—å —Å—Ç–∏–ª—å (–∑–º—ñ–Ω—ñ—Ç—å –∫–ª—é—á –¥–ª—è —ñ–Ω—à–æ–≥–æ —Å—Ç–∏–ª—é)\n",
        "SELECTED_STYLE = \"human_relatable\"  # –∞–±–æ \"humorous_casual\", \"technical_engaging\", \"discussion_starter\"\n",
        "\n",
        "# –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –æ–±—Ä–∞–Ω–æ–≥–æ —Å—Ç–∏–ª—é\n",
        "style_config = CONTENT_STYLES[SELECTED_STYLE]\n",
        "print(f\"üé≠ –û–±—Ä–∞–Ω–∏–π —Å—Ç–∏–ª—å: {style_config['description']}\")\n",
        "print(f\"üéØ –§–æ–∫—É—Å: {style_config['focus']}\")\n",
        "\n",
        "# –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å\n",
        "LLM_MODEL = style_config['model']\n",
        "MAX_TOKENS = style_config['max_tokens']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† LLM –ê–Ω–∞–ª—ñ–∑ —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ç–µ–Ω—Ç—É\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_and_generate_content(posts_df):\n",
        "    \"\"\"\n",
        "    –ê–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ LLM —Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö —ñ–¥–µ–π –ø–æ—Å—Ç—ñ–≤\n",
        "    \"\"\"\n",
        "    \n",
        "    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –¥–ª—è LLM\n",
        "    ANALYSIS_RULES = \"\"\"–£—è–≤–∏, —â–æ —Ç–∏ –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π Reddit-–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á –∑ –±–∞–≥–∞—Ç–æ—Ä—ñ—á–Ω–∏–º —Å—Ç–∞–∂–µ–º, —è–∫–∏–π —ñ–Ω—Ç—É—ó—Ç–∏–≤–Ω–æ –≤—ñ–¥—á—É–≤–∞—î, —â–æ \"–∑–∞–π–¥–µ\" —É —Å–ø—ñ–ª—å–Ω–æ—Ç—ñ. –¢–∏ –≤–º—ñ—î—à –ø–æ–º—ñ—á–∞—Ç–∏ —Ç–æ–Ω–∫—ñ –Ω—é–∞–Ω—Å–∏, —â–æ —Ä–æ–±–ª—è—Ç—å –ø–æ—Å—Ç –≤—ñ—Ä—É—Å–Ω–∏–º.\n",
        "\n",
        "    –¢–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è —è–∫ —Å–ø—Ä–∞–≤–∂–Ω—å–æ–≥–æ —Ä–µ–¥–¥—ñ—Ç–æ—Ä–∞:\n",
        "    1. üëÄ –ü–æ–º—ñ—Ç–∏—Ç–∏ –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω—ñ —Ç–µ–º–∏, —Ñ–æ—Ä–º–∞—Ç–∏ —á–∏ –∂–∞—Ä—Ç–∏ ‚Äî —â–æ –∑–∞—Ä–∞–∑ –Ω–∞–±–∏—Ä–∞—î –ø–æ–ø—É–ª—è—Ä–Ω—ñ—Å—Ç—å?\n",
        "    2. üß† –†–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏, —á–æ–º—É –¥–µ—è–∫—ñ –ø–æ—Å—Ç–∏ —Å—Ç–∞–ª–∏ —Ö—ñ—Ç–∞–º–∏ ‚Äî –æ—Å–æ–±–ª–∏–≤–∏–π —Å—Ç–∏–ª—å, –µ–º–æ—Ü—ñ—è, –æ—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è, –≥—É–º–æ—Ä?\n",
        "    3. ‚ú® –í–∏–≥–∞–¥–∞—Ç–∏ –Ω–æ–≤—ñ –ø–æ—Å—Ç–∏, —è–∫—ñ –∑–≤—É—á–∞—Ç—å –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ, —è–∫ –≤—ñ–¥ —Ä–µ–∞–ª—å–Ω–æ—ó –ª—é–¥–∏–Ω–∏ –∑ —Å–ø—Ä–∞–≤–∂–Ω—ñ–º–∏ –µ–º–æ—Ü—ñ—è–º–∏\n",
        "    \n",
        "    –ì–µ–Ω–µ—Ä—É–π –ø–æ—Å—Ç–∏, —è–∫—ñ:\n",
        "    - –ú–∞—é—Ç—å –æ—Å–æ–±–∏—Å—Ç–∏–π –¥–æ—Ç–∏–∫ (–¥–æ—Å–≤—ñ–¥, –ø—Ä–æ–±–ª–µ–º–∞, –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è)\n",
        "    - –í–∏–∫–ª–∏–∫–∞—é—Ç—å –µ–º–æ—Ü—ñ—ó (–∑–¥–∏–≤—É–≤–∞–Ω–Ω—è, —Å–ø—ñ–≤–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–Ω—è, –∑–∞—Ö–æ–ø–ª–µ–Ω–Ω—è, –≥—É–º–æ—Ä)\n",
        "    - –ó–≤—É—á–∞—Ç—å —è–∫ –∂–∏–≤–∞ –ª—é–¥–∏–Ω–∞, –∞ –Ω–µ –±–æ—Ç\n",
        "    - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –ø—Ä–∏—Ä–æ–¥–Ω—É Reddit-–º–æ–≤—É —Ç–∞ —Ç–æ–Ω —Å–ø—ñ–ª—å–Ω–æ—Ç–∏\n",
        "    \n",
        "    ‚ö†Ô∏è –í–ê–ñ–õ–ò–í–Ü –û–ë–ú–ï–ñ–ï–ù–ù–Ø:\n",
        "    - –ù–ï –≤–∏–≥–∞–¥—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø–æ–¥—ñ–π, —è–∫—ñ —Ç–æ—á–Ω–æ –Ω–µ –≤—ñ–¥–±—É–≤–∞–ª–∏—Å—è\n",
        "    - –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–±—É–¥–æ–≤–∞–Ω–∏–π –Ω–∞ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è—Ö, —Å—Ñ–æ—Ä–º—É–ª—é–π —Ü–µ —è–∫ –æ—Å–æ–±–∏—Å—Ç—É –¥—É–º–∫—É –∞–±–æ –≥—ñ–ø–æ—Ç–µ–∑—É\n",
        "    - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –µ–º–æ–¥–∑—ñ –ø–æ–º—ñ—Ä–Ω–æ: –Ω–µ –±—ñ–ª—å—à–µ 1-2 –≤ –æ–¥–Ω–æ–º—É –ø–æ—Å—Ç—ñ, —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –µ–º–æ—Ü—ñ—ó\n",
        "    - –û–¥–∏–Ω –∑ –¥–≤–æ—Ö –ø–æ—Å—Ç—ñ–≤ –û–ë–û–í'–Ø–ó–ö–û–í–û –º–∞—î –±—É—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–æ—é —ñ—Å—Ç–æ—Ä—ñ—î—é/—Å–ø–æ—Å—Ç–µ—Ä–µ–∂–µ–Ω–Ω—è–º, –∞ –Ω–µ –∑–∞–≥–∞–ª—å–Ω–æ—é –¥—É–º–∫–æ—é\n",
        "    \n",
        "    –§–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ JSON:\n",
        "    {\n",
        "        \"trends\": [\"—Ç—Ä–µ–Ω–¥1 (–∑ –ø–æ—è—Å–Ω–µ–Ω–Ω—è–º —á–æ–º—É –≤—ñ–Ω –ø–æ–ø—É–ª—è—Ä–Ω–∏–π)\", \"—Ç—Ä–µ–Ω–¥2\", \"—Ç—Ä–µ–Ω–¥3\", \"—Ç—Ä–µ–Ω–¥4\", \"—Ç—Ä–µ–Ω–¥5\"],\n",
        "        \"success_factors\": [\"—Ñ–∞–∫—Ç–æ—Ä1 (—â–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –ø—Ä–∞—Ü—é—î)\", \"—Ñ–∞–∫—Ç–æ—Ä2\", \"—Ñ–∞–∫—Ç–æ—Ä3\"],\n",
        "        \"post_ideas\": [\n",
        "            {\n",
        "                \"title\": \"–ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ—Å—Ç–∞ (–ø—Ä–∏—Ä–æ–¥–Ω–∏–π, —è–∫ –ø–∏—Å–∞–ª–∞ –± –∂–∏–≤–∞ –ª—é–¥–∏–Ω–∞)\",\n",
        "                \"content\": \"–û—Å–Ω–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –∑ –æ—Å–æ–±–∏—Å—Ç–∏–º —Ç–æ–Ω–æ–º, –µ–º–æ—Ü—ñ—è–º–∏ —Ç–∞ –¥–µ—Ç–∞–ª—è–º–∏, —â–æ –≤–∏–∫–ª–∏–∫–∞—é—Ç—å –≤—ñ–¥–≥—É–∫ (2-3 –∞–±–∑–∞—Ü–∏)\",\n",
        "                \"reasoning\": \"–ß–æ–º—É —Ü–µ–π –ø–æ—Å—Ç –∑–∞—Ü–µ–ø–∏—Ç—å –ª—é–¥–µ–π - –ø—Å–∏—Ö–æ–ª–æ–≥—ñ—è, –µ–º–æ—Ü—ñ—ó, –∞–∫—Ç—É–∞–ª—å–Ω—ñ—Å—Ç—å\",\n",
        "                \"tags\": [\"—Ç–µ–≥1\", \"—Ç–µ–≥2\", \"—Ç–µ–≥3\"],\n",
        "                \"estimated_engagement\": \"high/medium/low\",\n",
        "                \"post_type\": \"–æ—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è/–ø–∏—Ç–∞–Ω–Ω—è/–≥—É–º–æ—Ä/–ø–æ—Ä–∞–¥–∏/–¥–∏—Å–∫—É—Å—ñ—è\",\n",
        "                \"is_personal_story\": true,\n",
        "                \"safety_check\": \"–ü—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—é, —â–æ –Ω–µ –≤–∏–≥–∞–¥—É—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ñ–∞–∫—Ç—ñ–≤\"\n",
        "            },\n",
        "            {\n",
        "                \"title\": \"–î—Ä—É–≥–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—ñ–Ω—à–∏–π —Ç–æ–Ω —Ç–∞ —Ç–∏–ø - –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –≤—ñ–¥ –ø–µ—Ä—à–æ–≥–æ)\",\n",
        "                \"content\": \"–î—Ä—É–≥–∏–π –ø–æ—Å—Ç –∑ —ñ–Ω—à–∏–º –ø—ñ–¥—Ö–æ–¥–æ–º - —è–∫—â–æ –ø–µ—Ä—à–∏–π –æ—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è, —Ç–æ —Ü–µ–π –º–æ–∂–µ –±—É—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è–º —á–∏ –¥—É–º–∫–æ—é\",\n",
        "                \"reasoning\": \"–Ü–Ω—à–∞ –ø—Å–∏—Ö–æ–ª–æ–≥—ñ—á–Ω–∞ –º–æ—Ç–∏–≤–∞—Ü—ñ—è –¥–ª—è –∑–∞–ª—É—á–µ–Ω–Ω—è –∞—É–¥–∏—Ç–æ—Ä—ñ—ó\",\n",
        "                \"tags\": [\"—Ç–µ–≥1\", \"—Ç–µ–≥2\", \"—Ç–µ–≥3\"],\n",
        "                \"estimated_engagement\": \"high/medium/low\",\n",
        "                \"post_type\": \"–æ—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è/–ø–∏—Ç–∞–Ω–Ω—è/–≥—É–º–æ—Ä/–ø–æ—Ä–∞–¥–∏/–¥–∏—Å–∫—É—Å—ñ—è\",\n",
        "                \"is_personal_story\": false,\n",
        "                \"safety_check\": \"–ü—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—é, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é –æ—Å–æ–±–∏—Å—Ç—ñ –¥—É–º–∫–∏ —Ç–∞ –≥—ñ–ø–æ—Ç–µ–∑–∏\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    –ü–∏—à–∏ —è–∫ —Ä–µ–∞–ª—å–Ω–∞ –ª—é–¥–∏–Ω–∞, —è–∫–∞ –¥—ñ–ª–∏—Ç—å—Å—è –¥—É–º–∫–æ—é –∑ –¥—Ä—É–≥–æ–º —É Reddit. –ë–µ–∑ –ø–∞—Ñ–æ—Å—É. –ë–µ–∑ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É. –ü—Ä—è–º–æ, —â–∏—Ä–æ, —ñ–Ω–æ–¥—ñ –∑ —Å–∞–º–æ—ñ—Ä–æ–Ω—ñ—î—é.\n",
        "    \n",
        "    üí° –ü–†–ò–ö–õ–ê–î–ò –ü–†–ò–†–û–î–ù–ò–• REDDIT-–ó–ê–ì–û–õ–û–í–ö–Ü–í:\n",
        "    ‚Ä¢ –û—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è: \"–©–æ–π–Ω–æ –∑—Ä–æ–∑—É–º—ñ–≤, —â–æ 3 —Ä–æ–∫–∏ –ø–∏—Å–∞–≤ –∫–æ–¥ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ...\" / \"–ú—ñ–π –º–µ–Ω–µ–¥–∂–µ—Ä —Å–∫–∞–∑–∞–≤, —â–æ Python –ø–æ–≤—ñ–ª—å–Ω–∏–π. –î–æ–≤—ñ–≤ –π–æ–º—É –ø—Ä–æ—Ç–∏–ª–µ–∂–Ω–µ\"\n",
        "    ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∞: \"–•—Ç–æ—Å—å —â–µ –¥—É–º–∞—î, —â–æ GitHub Copilot —Ä–æ–±–∏—Ç—å –Ω–∞—Å –ª—ñ–Ω–∏–≤—ñ—à–∏–º–∏?\" / \"–Ø–∫ –≤–∏ —Å–ø—Ä–∞–≤–ª—è—î—Ç–µ—Å—å –∑ —Å–∏–Ω–¥—Ä–æ–º–æ–º —Å–∞–º–æ–∑–≤–∞–Ω—Ü—è –≤ IT?\"\n",
        "    ‚Ä¢ –í—ñ–¥–∫—Ä–∏—Ç—Ç—è: \"TIL —â–æ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —Ü–µ –≤ Python –æ–¥–Ω–∏–º —Ä—è–¥–∫–æ–º\" / \"–ü—Ä–æ—Å—Ç–∏–π —Ç—Ä—é–∫, —è–∫–∏–π –ø—Ä–∏—Å–∫–æ—Ä–∏–≤ –º—ñ–π –∫–æ–¥ —É 10 —Ä–∞–∑—ñ–≤\"\n",
        "    ‚Ä¢ –ì—É–º–æ—Ä: \"–ö–æ–ª–∏ –±–∞—á–∏—à –∫–æ–¥, —è–∫–∏–π –ø–∏—Å–∞–≤ 6 –º—ñ—Å—è—Ü—ñ–≤ —Ç–æ–º—É\" / \"–ú—ñ–π –∫–æ–¥ –ø—Ä–∞—Ü—é—î, –∞–ª–µ —è –Ω–µ –∑–Ω–∞—é —á–æ–º—É\"\n",
        "    ‚Ä¢ –î–∏—Å–∫—É—Å—ñ—è: \"–ù–µ–ø–æ–ø—É–ª—è—Ä–Ω–∞ –¥—É–º–∫–∞: TypeScript –ø–µ—Ä–µ–æ—Ü—ñ–Ω–µ–Ω–∏–π\" / \"–ó–º—ñ–Ω—ñ—Ç—å –º–æ—é –¥—É–º–∫—É: Python –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö –ø—Ä–æ–µ–∫—Ç—ñ–≤\"\n",
        "    \n",
        "    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ø—Ä–∏—Ä–æ–¥–Ω—ñ —Ñ—Ä–∞–∑–∏, –µ–º–æ—Ü—ñ—ó —Ç–∞ Reddit-—Å–ª–µ–Ω–≥. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–º JSON.\"\"\"\n",
        "    \n",
        "    async def analyze_single_subreddit(subreddit_name, subreddit_posts, client, semaphore):\n",
        "        @backoff.on_exception(backoff.expo, (openai.APIError, openai.RateLimitError, asyncio.TimeoutError), max_tries=3)\n",
        "        async def call():\n",
        "            async with semaphore:\n",
        "                # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É\n",
        "                top_posts = subreddit_posts.nlargest(5, 'score')\n",
        "                avg_score = subreddit_posts['score'].mean()\n",
        "                avg_comments = subreddit_posts['num_comments'].mean()\n",
        "                \n",
        "                # –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –ø–æ–ø—É–ª—è—Ä–Ω–∏—Ö —Å–ª—ñ–≤\n",
        "                all_titles = ' '.join(subreddit_posts['title'].tolist()).lower()\n",
        "                words = re.findall(r'\\\\b[a-zA-Z–∞-—è–ê-–Ø]{3,}\\\\b', all_titles)\n",
        "                common_words = [word for word, count in Counter(words).most_common(10)]\n",
        "                \n",
        "                user_prompt = f\"\"\"–û—Å—å –ø—ñ–¥–±—ñ—Ä–∫–∞ –ø–æ—Å—Ç—ñ–≤ –∑ r/{subreddit_name}. –ü–æ–¥–∏–≤–∏—Å—å, —â–æ —Ç—É—Ç –ø—Ä–∞—Ü—é—î!\n",
        "                \n",
        "                üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ø—ñ–ª—å–Ω–æ—Ç–∏:\n",
        "                ‚Ä¢ –ü–æ—Å—Ç—ñ–≤ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ: {len(subreddit_posts)}\n",
        "                ‚Ä¢ –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥: {avg_score:.1f} (–ø–æ–∫–∞–∑—É—î, –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –∞–∫—Ç–∏–≤–Ω–∞ —Å–ø—ñ–ª—å–Ω–æ—Ç–∞)\n",
        "                ‚Ä¢ –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤: {avg_comments:.1f} (—Ä—ñ–≤–µ–Ω—å –¥–∏—Å–∫—É—Å—ñ–π)\n",
        "                \n",
        "                üèÜ –ù–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à—ñ –ø–æ—Å—Ç–∏ (—â–æ —Ä–µ–∞–ª—å–Ω–æ –∑–∞–π—à–ª–æ –ª—é–¥—è–º):\n",
        "                {chr(10).join([f\"   {i+1}. \\\"{row['title']}\\\" ‚Äî {row['score']} –∞–ø–≤–æ—É—Ç—ñ–≤ üî•\" for i, (_, row) in enumerate(top_posts.iterrows())])}\n",
        "                \n",
        "                üî• –°–ª–æ–≤–∞, —â–æ —á–∞—Å—Ç–æ –∑'—è–≤–ª—è—é—Ç—å—Å—è: {', '.join(common_words[:10])}\n",
        "                (—Ü–µ –ø—ñ–¥–∫–∞–∑–∫–∏ –ø—Ä–æ —Ç–µ, —â–æ —Ü—ñ–∫–∞–≤–∏—Ç—å —Å–ø—ñ–ª—å–Ω–æ—Ç—É)\n",
        "                \n",
        "                –¢–µ–ø–µ—Ä —Ç–≤–æ—î –∑–∞–≤–¥–∞–Ω–Ω—è —è–∫ –¥–æ—Å–≤—ñ–¥—á–µ–Ω–æ–≥–æ —Ä–µ–¥–¥—ñ—Ç–æ—Ä–∞:\n",
        "                –ü–æ–¥—É–º–∞–π ‚Äî —â–æ —Ä–æ–±–∏—Ç—å —Ü—ñ –ø–æ—Å—Ç–∏ —É—Å–ø—ñ—à–Ω–∏–º–∏? –Ø–∫–∞ –µ–º–æ—Ü—ñ—è, –ø—Ä–æ–±–ª–µ–º–∞ —á–∏ —Ü—ñ–∫–∞–≤–∏–Ω–∫–∞ –∑–∞—á–µ–ø–∏–ª–∞ –ª—é–¥–µ–π?\n",
        "                \n",
        "                –ó–≥–µ–Ω–µ—Ä—É–π 2 —ñ–¥–µ—ó –ø–æ—Å—Ç—ñ–≤ –¥–ª—è r/{subreddit_name}, —è–∫—ñ:\n",
        "                ‚Ä¢ –ó–≤—É—á–∞—Ç—å —è–∫ –≤—ñ–¥ —Å–ø—Ä–∞–≤–∂–Ω—å–æ—ó –ª—é–¥–∏–Ω–∏ –∑ —Ä–µ–∞–ª—å–Ω–æ—é –ø—Ä–æ–±–ª–µ–º–æ—é/–¥–æ—Å–≤—ñ–¥–æ–º\n",
        "                ‚Ä¢ –ú–∞—é—Ç—å –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –µ–º–æ—Ü—ñ—ó —Ç–∞ –¥–∏—Å–∫—É—Å—ñ—ó  \n",
        "                ‚Ä¢ –í–ø–∏—Å—É—é—Ç—å—Å—è –≤ –∫—É–ª—å—Ç—É—Ä—É —Ü—ñ—î—ó —Å–ø—ñ–ª—å–Ω–æ—Ç–∏\n",
        "                ‚Ä¢ –ù–ï –ø–æ–≤—Ç–æ—Ä—é—é—Ç—å —ñ—Å–Ω—É—é—á—ñ –ø–æ—Å—Ç–∏, –∞ –ø—Ä–æ–ø–æ–Ω—É—é—Ç—å —â–æ—Å—å —Å–≤—ñ–∂–µ\n",
        "                \n",
        "                ‚ö†Ô∏è –û–ë–û–í'–Ø–ó–ö–û–í–Ü –í–ò–ú–û–ì–ò:\n",
        "                ‚Ä¢ –û–¥–∏–Ω –ø–æ—Å—Ç –º–∞—î –±—É—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–æ—é —ñ—Å—Ç–æ—Ä—ñ—î—é/–¥–æ—Å–≤—ñ–¥–æ–º (–Ω–µ –∑–∞–≥–∞–ª—å–Ω–æ—é –¥—É–º–∫–æ—é)\n",
        "                ‚Ä¢ –ù–µ –≤–∏–≥–∞–¥—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ñ–∞–∫—Ç—ñ–≤ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –≥—ñ–ø–æ—Ç–µ–∑–∏ —Ç–∞ –æ—Å–æ–±–∏—Å—Ç—ñ –¥—É–º–∫–∏\n",
        "                ‚Ä¢ –ï–º–æ–¥–∑—ñ —Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø—ñ–¥—Å–∏–ª–µ–Ω–Ω—è –µ–º–æ—Ü—ñ—ó (–º–∞–∫—Å–∏–º—É–º 1-2 –Ω–∞ –ø–æ—Å—Ç)\"\"\"\n",
        "                \n",
        "                # –ü–µ—Ä—à–∏–π –∑–∞–ø–∏—Ç\n",
        "                response = await asyncio.wait_for(\n",
        "                    client.chat.completions.create(\n",
        "                        model=LLM_MODEL,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": ANALYSIS_RULES},\n",
        "                            {\"role\": \"user\", \"content\": user_prompt}\n",
        "                        ],\n",
        "                        temperature=style_config['temperature'],\n",
        "                        max_tokens=MAX_TOKENS,\n",
        "                        stream=False\n",
        "                    ),\n",
        "                    timeout=TIMEOUT_SECONDS\n",
        "                )\n",
        "                \n",
        "                result = response.choices[0].message.content.strip()\n",
        "                \n",
        "                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –æ–±—Ä—ñ–∑–∞–Ω–Ω—è —Ç–∞ –¥–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è\n",
        "                if ENABLE_CONTINUATION and response.choices[0].finish_reason == 'length':\n",
        "                    print(f\"‚ö†Ô∏è –í—ñ–¥–ø–æ–≤—ñ–¥—å –æ–±—Ä—ñ–∑–∞–Ω–∞ –¥–ª—è r/{subreddit_name}, –≤–∏–∫–æ–Ω—É—é –¥–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—é...\")\n",
        "                    \n",
        "                    # –î–æ–≥–µ–Ω–µ—Ä–∞—Ü—ñ—è\n",
        "                    continue_response = await asyncio.wait_for(\n",
        "                        client.chat.completions.create(\n",
        "                            model=LLM_MODEL,\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"–ü—Ä–æ–¥–æ–≤–∂–∏ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ —Ç–æ–≥–æ –º—ñ—Å—Ü—è, –¥–µ –≤–æ–Ω–∞ –±—É–ª–∞ –æ–±—Ä—ñ–∑–∞–Ω–∞. –í–µ—Ä–Ω–∏ –¢–Ü–õ–¨–ö–ò –≤–∞–ª—ñ–¥–Ω–∏–π JSON.\"},\n",
        "                                {\"role\": \"user\", \"content\": f\"–û–±—Ä—ñ–∑–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {result}\\\\n\\\\n–ü—Ä–æ–¥–æ–≤–∂–∏ —Ç–∞ –∑–∞–≤–µ—Ä—à–∏—Ç–∏ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É.\"}\n",
        "                            ],\n",
        "                            temperature=style_config['temperature'],\n",
        "                            max_tokens=MAX_TOKENS // 2,\n",
        "                            stream=False\n",
        "                        ),\n",
        "                        timeout=TIMEOUT_SECONDS // 2\n",
        "                    )\n",
        "                    \n",
        "                    # –°–ø—Ä–æ–±–∞ –æ–±'—î–¥–Ω–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ\n",
        "                    continued_result = continue_response.choices[0].message.content.strip()\n",
        "                    \n",
        "                    # –ü—Ä–æ—Å—Ç–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±'—î–¥–Ω–∞–Ω–Ω—è JSON\n",
        "                    if result.endswith('...') or not result.endswith('}'):\n",
        "                        # –í–∏–¥–∞–ª—è—î–º–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω—ñ —á–∞—Å—Ç–∏–Ω–∏ —Ç–∞ –æ–±'—î–¥–Ω—É—î–º–æ\n",
        "                        if '\"post_ideas\":' in result and not result.rstrip().endswith(']}'):\n",
        "                            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å—Ç–∞–Ω–Ω—é –ø–æ–≤–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
        "                            try:\n",
        "                                # –°–ø—Ä–æ–±—É—î–º–æ –ø–∞—Ä—Å–∏—Ç–∏ continued_result —è–∫ –ø–æ–≤–Ω–∏–π JSON\n",
        "                                import json\n",
        "                                json.loads(continued_result)\n",
        "                                result = continued_result\n",
        "                            except:\n",
        "                                # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "                                pass\n",
        "                \n",
        "                return {\n",
        "                    \"subreddit\": subreddit_name,\n",
        "                    \"result\": result,\n",
        "                    \"finish_reason\": response.choices[0].finish_reason\n",
        "                }\n",
        "        \n",
        "        return await call()\n",
        "    \n",
        "    async def process_all_subreddits(subreddits_data):\n",
        "        client = openai.AsyncOpenAI(\n",
        "            api_key=OPENROUTER_API_KEY,\n",
        "            base_url=\"https://openrouter.ai/api/v1\"\n",
        "        )\n",
        "        semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "        \n",
        "        tasks = [\n",
        "            analyze_single_subreddit(subreddit, data, client, semaphore)\n",
        "            for subreddit, data in subreddits_data.items()\n",
        "        ]\n",
        "        \n",
        "        results = []\n",
        "        completed = 0\n",
        "        total = len(tasks)\n",
        "        \n",
        "        for task in asyncio.as_completed(tasks):\n",
        "            result = await task\n",
        "            results.append(result)\n",
        "            completed += 1\n",
        "            print(f\"Overall: {completed/total*100:.1f}% | Completed: {completed}/{total}\")\n",
        "        \n",
        "        await client.close()\n",
        "        return results\n",
        "    \n",
        "    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –ø–æ —Å–∞–±—Ä–µ–¥–¥—ñ—Ç–∞–º\n",
        "    subreddits_data = {}\n",
        "    for subreddit in CONTENT_GENERATION_SUBREDDITS:\n",
        "        subreddit_posts = posts_df[posts_df['subreddit'] == subreddit]\n",
        "        if len(subreddit_posts) > 0:\n",
        "            subreddits_data[subreddit] = subreddit_posts\n",
        "    \n",
        "    print(f\"üß† –ó–∞–ø—É—Å–∫ LLM –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è {len(subreddits_data)} —Å–∞–±—Ä–µ–¥–¥—ñ—Ç—ñ–≤...\")\n",
        "    \n",
        "    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É\n",
        "    loop = asyncio.get_event_loop()\n",
        "    results = loop.run_until_complete(process_all_subreddits(subreddits_data))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª—ñ–∑—É\n",
        "analysis_results = analyze_and_generate_content(posts_df)\n",
        "\n",
        "print(f\"\\n‚úÖ LLM –∞–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –¥–ª—è {len(analysis_results)} —Å–∞–±—Ä–µ–¥–¥—ñ—Ç—ñ–≤\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∫–æ–Ω—Ç–µ–Ω—Ç—É\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É JSON\n",
        "def parse_json_response(response_text, subreddit_name):\n",
        "    \"\"\"\n",
        "    –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –∑ –æ–±—Ä–æ–±–∫–æ—é —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # –°–ø–æ—á–∞—Ç–∫—É —Å–ø—Ä–æ–±—É—î–º–æ –ø–∞—Ä—Å–∏—Ç–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —è–∫ JSON\n",
        "        data = json.loads(response_text)\n",
        "        return data, True\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    \n",
        "    # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ JSON –±–ª–æ–∫ —É —Ç–µ–∫—Å—Ç—ñ\n",
        "    json_patterns = [\n",
        "        r'```json\\\\s*({.*?})\\\\s*```',  # JSON —É markdown –±–ª–æ—Ü—ñ\n",
        "        r'({[^{}]*(?:{[^{}]*}[^{}]*)*})',  # –ü—Ä–æ—Å—Ç–∏–π JSON –±–ª–æ–∫\n",
        "        r'({.*})',  # –ë—É–¥—å-—è–∫–∏–π –±–ª–æ–∫ —É —Ñ—ñ–≥—É—Ä–Ω–∏—Ö –¥—É–∂–∫–∞—Ö\n",
        "    ]\n",
        "    \n",
        "    for pattern in json_patterns:\n",
        "        matches = re.findall(pattern, response_text, re.DOTALL)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                # –û—á–∏—â—É—î–º–æ JSON –≤—ñ–¥ –º–æ–∂–ª–∏–≤–∏—Ö –ø—Ä–æ–±–ª–µ–º\n",
        "                clean_json = match.strip()\n",
        "                \n",
        "                # –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –ø–æ—à–∏—Ä–µ–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏\n",
        "                clean_json = re.sub(r',\\\\s*}', '}', clean_json)  # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –∫–æ–º–∏\n",
        "                clean_json = re.sub(r',\\\\s*]', ']', clean_json)  # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –∫–æ–º–∏ –≤ –º–∞—Å–∏–≤–∞—Ö\n",
        "                \n",
        "                # –Ø–∫—â–æ JSON –æ–±—Ä—ñ–∑–∞–Ω–∏–π, —Å–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–µ—Ä—à–∏—Ç–∏ –π–æ–≥–æ\n",
        "                if not clean_json.endswith('}') and '\"post_ideas\"' in clean_json:\n",
        "                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Å—Ç–∞–Ω–Ω—é –ø–æ–≤–Ω—É —ñ–¥–µ—é\n",
        "                    ideas_start = clean_json.find('\"post_ideas\": [')\n",
        "                    if ideas_start != -1:\n",
        "                        # –†–∞—Ö—É—î–º–æ –≤—ñ–¥–∫—Ä–∏—Ç—ñ –¥—É–∂–∫–∏\n",
        "                        bracket_count = 0\n",
        "                        last_complete_pos = ideas_start\n",
        "                        \n",
        "                        for i, char in enumerate(clean_json[ideas_start:], ideas_start):\n",
        "                            if char == '{':\n",
        "                                bracket_count += 1\n",
        "                            elif char == '}':\n",
        "                                bracket_count -= 1\n",
        "                                if bracket_count == 0:\n",
        "                                    last_complete_pos = i + 1\n",
        "                        \n",
        "                        # –û–±—Ä—ñ–∑–∞—î–º–æ –¥–æ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –ø–æ–≤–Ω–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏\n",
        "                        if last_complete_pos > ideas_start:\n",
        "                            before_ideas = clean_json[:ideas_start + len('\"post_ideas\": [')]\n",
        "                            ideas_part = clean_json[ideas_start + len('\"post_ideas\": ['):last_complete_pos]\n",
        "                            clean_json = before_ideas + ideas_part + ']}'\n",
        "                \n",
        "                data = json.loads(clean_json)\n",
        "                return data, True\n",
        "                \n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    \n",
        "    # –Ø–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –ø–∞—Ä—Å–∏—Ç–∏, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ None\n",
        "    return None, False\n",
        "\n",
        "# –û–±—Ä–æ–±–∫–∞ —Ç–∞ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
        "generated_ideas = []\n",
        "\n",
        "for result in analysis_results:\n",
        "    subreddit = result['subreddit']\n",
        "    response_text = result['result']\n",
        "    finish_reason = result.get('finish_reason', 'unknown')\n",
        "    \n",
        "    print(f\"\\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–ò –î–õ–Ø r/{subreddit}:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if finish_reason == 'length':\n",
        "        print(\"‚ö†Ô∏è –í—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –æ–±—Ä—ñ–∑–∞–Ω–∞ —á–µ—Ä–µ–∑ –ª—ñ–º—ñ—Ç —Ç–æ–∫–µ–Ω—ñ–≤\")\n",
        "    \n",
        "    # –°–ø—Ä–æ–±–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥—É\n",
        "    data, success = parse_json_response(response_text, subreddit)\n",
        "    \n",
        "    if success and data:\n",
        "        # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç—Ä–µ–Ω–¥—ñ–≤\n",
        "        if 'trends' in data:\n",
        "            print(f\"üìà –¢—Ä–µ–Ω–¥–∏ ({len(data['trends'])}): {', '.join(data['trends'])}\")\n",
        "        \n",
        "        # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ñ–∞–∫—Ç–æ—Ä—ñ–≤ —É—Å–ø—ñ—Ö—É\n",
        "        if 'success_factors' in data:\n",
        "            print(f\"üéØ –§–∞–∫—Ç–æ—Ä–∏ —É—Å–ø—ñ—Ö—É ({len(data['success_factors'])}): {', '.join(data['success_factors'])}\")\n",
        "        \n",
        "        # –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è —ñ–¥–µ–π –ø–æ—Å—Ç—ñ–≤\n",
        "        if 'post_ideas' in data and data['post_ideas']:\n",
        "            print(f\"\\nüí° –ó–ì–ï–ù–ï–†–û–í–ê–ù–Ü –Ü–î–ï–á ({len(data['post_ideas'])}):\\\\n\")\n",
        "            \n",
        "            # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –≤–∏–º–æ–≥\n",
        "            personal_stories = sum(1 for idea in data['post_ideas'] if idea.get('is_personal_story', False))\n",
        "            if personal_stories == 0:\n",
        "                print(\"‚ö†Ô∏è –£–í–ê–ì–ê: –ñ–æ–¥–Ω–∞ —ñ–¥–µ—è –Ω–µ –ø–æ–∑–Ω–∞—á–µ–Ω–∞ —è–∫ –æ—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è!\")\n",
        "            elif personal_stories >= 1:\n",
        "                print(f\"‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞: {personal_stories} –æ—Å–æ–±–∏—Å—Ç–∏—Ö —ñ—Å—Ç–æ—Ä—ñ–π –∑–Ω–∞–π–¥–µ–Ω–æ\")\n",
        "            print()\n",
        "            \n",
        "            for i, idea in enumerate(data['post_ideas'], 1):\n",
        "                print(f\"üí° –Ü–¥–µ—è {i}:\")\n",
        "                print(f\"  üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫: {idea.get('title', 'N/A')}\")\n",
        "                \n",
        "                content = idea.get('content', 'N/A')\n",
        "                if len(content) > 150:\n",
        "                    print(f\"  üìÑ –ö–æ–Ω—Ç–µ–Ω—Ç: {content[:150]}...\")\n",
        "                else:\n",
        "                    print(f\"  üìÑ –ö–æ–Ω—Ç–µ–Ω—Ç: {content}\")\n",
        "                \n",
        "                print(f\"  üéØ –û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è: {idea.get('reasoning', 'N/A')}\")\n",
        "                print(f\"  üè∑Ô∏è –¢–µ–≥–∏: {', '.join(idea.get('tags', []))}\")\n",
        "                print(f\"  üìà –ü—Ä–æ–≥–Ω–æ–∑ –∑–∞–ª—É—á–µ–Ω–æ—Å—Ç—ñ: {idea.get('estimated_engagement', 'N/A')}\")\n",
        "                \n",
        "                # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –±–µ–∑–ø–µ–∫–∏ —Ç–∞ —Ç–∏–ø—É\n",
        "                post_type = idea.get('post_type', 'N/A')\n",
        "                is_personal = idea.get('is_personal_story', False)\n",
        "                safety_check = idea.get('safety_check', 'N/A')\n",
        "                \n",
        "                print(f\"  üé≠ –¢–∏–ø –ø–æ—Å—Ç–∞: {post_type}\")\n",
        "                print(f\"  üë§ –û—Å–æ–±–∏—Å—Ç–∞ —ñ—Å—Ç–æ—Ä—ñ—è: {'‚úÖ –¢–∞–∫' if is_personal else '‚ùå –ù—ñ'}\")\n",
        "                print(f\"  üõ°Ô∏è –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–µ–∑–ø–µ–∫–∏: {safety_check}\")\n",
        "                print()\n",
        "                \n",
        "                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É\n",
        "                idea_data = idea.copy()\n",
        "                idea_data['target_subreddit'] = subreddit\n",
        "                generated_ideas.append(idea_data)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —ñ–¥–µ–π –ø–æ—Å—Ç—ñ–≤ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ\")\n",
        "            \n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –ø–∞—Ä—Å–∏—Ç–∏ JSON, –ø–æ–∫–∞–∑—É—é —Å–∏—Ä–∏–π —Ç–µ–∫—Å—Ç:\")\n",
        "        print(\"=\" * 30)\n",
        "        print(response_text[:800] + (\"...\" if len(response_text) > 800 else \"\"))\n",
        "        print(\"=\" * 30)\n",
        "        \n",
        "        # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏—Ç—è–≥—Ç–∏ —Ö–æ—á–∞ –± —á–∞—Å—Ç–∏–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó\n",
        "        if \"trends\" in response_text.lower():\n",
        "            print(\"\\\\nüìà –ó–Ω–∞–π–¥–µ–Ω–æ –∑–≥–∞–¥–∫–∏ —Ç—Ä–µ–Ω–¥—ñ–≤ —É —Ç–µ–∫—Å—Ç—ñ\")\n",
        "        if \"post_ideas\" in response_text.lower():\n",
        "            print(\"üí° –ó–Ω–∞–π–¥–µ–Ω–æ –∑–≥–∞–¥–∫–∏ —ñ–¥–µ–π –ø–æ—Å—Ç—ñ–≤ —É —Ç–µ–∫—Å—Ç—ñ\")\n",
        "\n",
        "print(f\"\\nüéâ –ó–∞–≥–∞–ª–æ–º –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(generated_ideas)} —ñ–¥–µ–π –ø–æ—Å—Ç—ñ–≤!\")\n",
        "\n",
        "if len(generated_ideas) == 0:\n",
        "    print(\"\\\\nüí° –ü–û–†–ê–î–ò –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í:\")\n",
        "    print(\"1. –ó–±—ñ–ª—å—à—Ç–µ MAX_TOKENS –¥–æ 3000+ —É –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö\")\n",
        "    print(\"2. –°–ø—Ä–æ–±—É–π—Ç–µ —ñ–Ω—à—É LLM –º–æ–¥–µ–ª—å (anthropic/claude-3-haiku)\")\n",
        "    print(\"3. –ó–º–µ–Ω—à—Ç–µ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∞–±—Ä–µ–¥–¥—ñ—Ç—ñ–≤ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è\")\n",
        "    print(\"4. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —è–∫—ñ—Å—Ç—å —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∑'—î–¥–Ω–∞–Ω–Ω—è\")\n",
        "    print(\"5. –°–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –∞–Ω–∞–ª—ñ–∑ —â–µ —Ä–∞–∑\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è timestamp –¥–ª—è —Ñ–∞–π–ª—ñ–≤\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ—Å—Ç—ñ–≤ —É Excel\n",
        "# posts_filename = f\"reddit_posts_{timestamp}.xlsx\"\n",
        "# with pd.ExcelWriter(posts_filename, engine='openpyxl') as writer:\n",
        "#     posts_df.to_excel(writer, sheet_name='Posts', index=False)\n",
        "#     if len(comments_df) > 0:\n",
        "#         comments_df.to_excel(writer, sheet_name='Comments', index=False)\n",
        "    \n",
        "#     # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∞–±—Ä–µ–¥–¥—ñ—Ç–∞–º\n",
        "#     subreddit_stats = posts_df.groupby('subreddit').agg({\n",
        "#         'score': ['mean', 'max', 'min'],\n",
        "#         'num_comments': ['mean', 'max'],\n",
        "#         'post_id': 'count',\n",
        "#         'upvote_ratio': 'mean'\n",
        "#     }).round(2)\n",
        "#     subreddit_stats.to_excel(writer, sheet_name='Subreddit_Stats')\n",
        "\n",
        "# print(f\"‚úÖ –ü–æ—Å—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {posts_filename}\")\n",
        "\n",
        "# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏—Ö —ñ–¥–µ–π\n",
        "if generated_ideas:\n",
        "    ideas_filename = f\"generated_ideas_{timestamp}.json\"\n",
        "    \n",
        "    export_data = {\n",
        "        \"metadata\": {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"total_ideas\": len(generated_ideas),\n",
        "            \"target_subreddits\": CONTENT_GENERATION_SUBREDDITS,\n",
        "            \"source_posts\": len(posts_df),\n",
        "            \"llm_model\": LLM_MODEL\n",
        "        },\n",
        "        \"generated_ideas\": generated_ideas,\n",
        "        \"source_analysis\": {\n",
        "            \"total_posts\": len(posts_df),\n",
        "            \"avg_score\": float(posts_df['score'].mean()),\n",
        "            \"avg_comments\": float(posts_df['num_comments'].mean()),\n",
        "            \"subreddits\": posts_df['subreddit'].unique().tolist()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    with open(ideas_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"‚úÖ –Ü–¥–µ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {ideas_filename}\")\n",
        "\n",
        "print(f\"\\nüéâ –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "print(f\"üìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏:\")\n",
        "# print(f\"  üìä {posts_filename} - –î–∞–Ω—ñ –ø–æ—Å—Ç—ñ–≤ —Ç–∞ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ–≤\")\n",
        "if generated_ideas:\n",
        "    print(f\"  üí° {ideas_filename} - –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —ñ–¥–µ—ó (JSON)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç –î–æ–¥–∞—Ç–∫–æ–≤–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª—ñ–∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö —á–∞—Å—ñ–≤ –¥–ª—è –ø–æ—Å—Ç–∏–Ω–≥—É\n",
        "posts_df['hour'] = pd.to_datetime(posts_df['created_utc']).dt.hour\n",
        "posts_df['day_of_week'] = pd.to_datetime(posts_df['created_utc']).dt.day_name()\n",
        "\n",
        "print(\"‚è∞ –ù–ê–ô–ö–†–ê–©–Ü –ì–û–î–ò–ù–ò –î–õ–Ø –ü–û–°–¢–ò–ù–ì–£ (–∑–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–º):\")\n",
        "hourly_stats = posts_df.groupby('hour')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
        "for hour, stats in hourly_stats.head(5).iterrows():\n",
        "    print(f\"  {hour:02d}:00 - –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥: {stats['mean']:.1f} ({stats['count']} –ø–æ—Å—Ç—ñ–≤)\")\n",
        "\n",
        "print(\"\\nüìÖ –ù–ê–ô–ö–†–ê–©–Ü –î–ù–Ü –¢–ò–ñ–ù–Ø:\")\n",
        "daily_stats = posts_df.groupby('day_of_week')['score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
        "for day, stats in daily_stats.iterrows():\n",
        "    print(f\"  {day}: {stats['mean']:.1f} —Å–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–π—Ç–∏–Ω–≥ ({stats['count']} –ø–æ—Å—Ç—ñ–≤)\")\n",
        "\n",
        "# –ê–Ω–∞–ª—ñ–∑ –¥–æ–≤–∂–∏–Ω–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤\n",
        "posts_df['title_length'] = posts_df['title'].str.len()\n",
        "print(f\"\\nüìù –ê–ù–ê–õ–Ü–ó –ó–ê–ì–û–õ–û–í–ö–Ü–í:\")\n",
        "print(f\"  –°–µ—Ä–µ–¥–Ω—è –¥–æ–≤–∂–∏–Ω–∞: {posts_df['title_length'].mean():.1f} —Å–∏–º–≤–æ–ª—ñ–≤\")\n",
        "print(f\"  –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ (—Ç–æ–ø 25%): {posts_df.nlargest(len(posts_df)//4, 'score')['title_length'].mean():.1f} —Å–∏–º–≤–æ–ª—ñ–≤\")\n",
        "\n",
        "# –ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∏–π –∞–Ω–∞–ª—ñ–∑\n",
        "numeric_cols = ['score', 'num_comments', 'upvote_ratio', 'title_length', 'selftext_length']\n",
        "correlation_matrix = posts_df[numeric_cols].corr()\n",
        "\n",
        "print(f\"\\nüîó –ö–û–†–ï–õ–Ø–¶–Ü–á (–∑ —Ä–µ–π—Ç–∏–Ω–≥–æ–º):\")\n",
        "score_correlations = correlation_matrix['score'].sort_values(ascending=False)[1:]  # –í–∏–∫–ª—é—á–∞—î–º–æ —Å–∞–º—É —Å–µ–±–µ\n",
        "for feature, corr in score_correlations.items():\n",
        "    print(f\"  {feature}: {corr:.3f}\")\n",
        "\n",
        "print(f\"\\nüìä –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
